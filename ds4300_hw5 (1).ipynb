{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ec3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb40562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the spark session and has magic code that makes things not break\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark = SparkSession.builder.appName('hw5').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259c44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "# Initiailizes the udf version of the haversine function\n",
    "haversineUDF = udf(lambda x, y: haversine(28.396837, -80.605659, x, y), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9538d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the stations from the csv\n",
    "stations = spark.read.csv('stations.csv', header=False, inferSchema=True).withColumnRenamed(\"_c0\", \"StationID\").withColumnRenamed(\"_c1\", \"WBANID\").withColumnRenamed(\"_c2\", \"GPSLatitude\").withColumnRenamed(\"_c3\", \"GPSLongitude\")\n",
    "\n",
    "# Creates a df with the distance from Cape Canaveral using the haversine function\n",
    "stations = stations.withColumn(\"distance\", haversineUDF(stations.GPSLatitude, stations.GPSLongitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67edc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.createOrReplaceTempView('stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab23db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+------------+---------+\n",
      "|StationID|WBANID|GPSLatitude|GPSLongitude| distance|\n",
      "+---------+------+-----------+------------+---------+\n",
      "|   722050| 12815|     28.434|     -81.325| 70.47143|\n",
      "|   722053| 12841|     28.545|     -81.333| 72.97912|\n",
      "|   747946| 12886|     28.617|     -80.683|25.620928|\n",
      "|   747950| 12867|     28.233|       -80.6|18.226263|\n",
      "|   722040| 12838|     28.101|     -80.644|33.109257|\n",
      "|   749047|  null|     28.283|     -81.416|    80.31|\n",
      "|   997806|  null|       28.4|     -80.533|7.1157584|\n",
      "|   720904|   299|     29.067|     -81.283| 99.57265|\n",
      "|   722056| 12834|     29.183|     -81.048| 97.46734|\n",
      "|   722051| 12841|     28.545|     -81.333| 72.97912|\n",
      "|   747945|  null|     28.617|       -80.7|  26.1591|\n",
      "|   998275|  null|     28.017|     -80.683|  42.9105|\n",
      "|   747870| 12834|     29.183|     -81.048| 97.46734|\n",
      "|   997354|  null|      28.42|      -80.58|3.5960674|\n",
      "|   995450|  null|     28.519|     -80.166| 45.07606|\n",
      "|   722046| 12898|     28.517|       -80.8|23.226759|\n",
      "|   747940| 12868|     28.483|     -80.567|10.299568|\n",
      "|   722058|  null|      29.07|      -80.92|80.883995|\n",
      "|   722361| 92808|     29.054|     -80.948| 80.33688|\n",
      "|   722011| 92813|      28.29|     -81.437| 82.22145|\n",
      "+---------+------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removes stations that have null coordinates or IDs, removes stations that are more than 100km away from Cape Canaveral\n",
    "# and drops duplicates\n",
    "stations = spark.sql(\"\"\"\n",
    "    SELECT * FROM STATIONS\n",
    "    WHERE GPSLatitude != 0 AND GPSLongitude != 0 AND distance <= 100 AND StationID IS NOT NULL\n",
    "\"\"\")\n",
    "stations = stations.dropDuplicates([\"StationID\"])\n",
    "stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6cc113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+---+----+\n",
      "|StationID|WBANID|Month|Day|temp|\n",
      "+---------+------+-----+---+----+\n",
      "|    10010|  null|    1|  1|17.2|\n",
      "|    10010|  null|    1|  2|12.1|\n",
      "|    10010|  null|    1|  3|10.4|\n",
      "|    10010|  null|    1|  4|17.4|\n",
      "|    10010|  null|    1|  5|26.5|\n",
      "|    10010|  null|    1|  6|30.1|\n",
      "|    10010|  null|    1|  7|29.7|\n",
      "|    10010|  null|    1|  8|29.6|\n",
      "|    10010|  null|    1|  9|29.6|\n",
      "|    10010|  null|    1| 10|33.0|\n",
      "|    10010|  null|    1| 11|32.5|\n",
      "|    10010|  null|    1| 12|27.4|\n",
      "|    10010|  null|    1| 13|22.2|\n",
      "|    10010|  null|    1| 14|11.3|\n",
      "|    10010|  null|    1| 15| 2.5|\n",
      "|    10010|  null|    1| 16| 3.0|\n",
      "|    10010|  null|    1| 17|13.4|\n",
      "|    10010|  null|    1| 18|29.8|\n",
      "|    10010|  null|    1| 19|27.5|\n",
      "|    10010|  null|    1| 20|25.2|\n",
      "+---------+------+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1986 = spark.read.csv('1986.csv', header=False, inferSchema=True).withColumnRenamed(\"_c0\", \"StationID\").withColumnRenamed(\"_c1\", \"WBANID\").withColumnRenamed(\"_c2\", \"Month\").withColumnRenamed(\"_c3\", \"Day\").withColumnRenamed(\"_c4\", \"temp\")\n",
    "data_1986.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ff4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1986.createOrReplaceTempView('_1986')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8fbb3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+---+----+\n",
      "|StationID|WBANID|Month|Day|temp|\n",
      "+---------+------+-----+---+----+\n",
      "|    10010|  null|    1| 28|14.1|\n",
      "|    10080|  null|    1| 28|32.3|\n",
      "|    10100|  null|    1| 28|30.9|\n",
      "|    10230|  null|    1| 28|27.2|\n",
      "|    10250|  null|    1| 28|32.5|\n",
      "|    10280|  null|    1| 28|33.0|\n",
      "|    10330|  null|    1| 28|34.0|\n",
      "|    10350|  null|    1| 28|31.0|\n",
      "|    10470|  null|    1| 28|22.1|\n",
      "|    10490|  null|    1| 28|30.7|\n",
      "|    10520|  null|    1| 28|29.9|\n",
      "|    10530|  null|    1| 28|29.8|\n",
      "|    10550|  null|    1| 28|31.6|\n",
      "|    10590|  null|    1| 28|29.2|\n",
      "|    10620|  null|    1| 28|30.5|\n",
      "|    10630|  null|    1| 28|28.2|\n",
      "|    10650|  null|    1| 28|26.1|\n",
      "|    10680|  null|    1| 28|31.8|\n",
      "|    10740|  null|    1| 28|28.5|\n",
      "|    10780|  null|    1| 28|30.4|\n",
      "+---------+------+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jan28 = spark.sql(\"\"\"\n",
    "    SELECT * FROM _1986\n",
    "    WHERE Month == 1 AND Day == 28\n",
    "\"\"\")\n",
    "jan28.show()\n",
    "jan28.createOrReplaceTempView('jan28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd4db47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='_1986', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='jan28', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='stations', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2753dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----+\n",
      "|StationID| distance|temp|\n",
      "+---------+---------+----+\n",
      "|   722040|33.109257|33.7|\n",
      "|   722045| 90.04572|37.5|\n",
      "|   722046|23.226759|37.0|\n",
      "|   722050| 70.47143|34.7|\n",
      "|   722051| 72.97912|15.3|\n",
      "|   722056| 97.46734|31.8|\n",
      "|   722057| 75.49695|33.4|\n",
      "|   747945|  26.1591|33.7|\n",
      "|   747950|18.226263|39.6|\n",
      "+---------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = stations.join(jan28, stations.StationID == jan28.StationID, \"inner\").select(stations.StationID, stations.distance, jan28.temp)\n",
    "joined_data.createOrReplaceTempView('joined')\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f60968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((12/350) + (10/750) + (10/850)) / ((1/350) + (1/750) + (1/850))\n",
    "def idw_temp_est(distances, temps):\n",
    "    inv_dists = [1/dist for dist in distances]\n",
    "    temp_dist = [temp/dist for temp, dist in (temps, distances)]\n",
    "    \n",
    "    num = sum(temp_dist)\n",
    "    denom = sum(inv_dists)\n",
    "    \n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af3c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_dist(x):\n",
    "    return 1/x\n",
    "\n",
    "def temp_inv_dist(inv_dist, temp):\n",
    "    return temp * inv_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db8b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = joined_data.withColumn(\"inv_dist\", inv_dist(joined_data.distance))\n",
    "joined_data = joined_data.withColumn(\"temp_dist\",temp_inv_dist(joined_data.inv_dist,joined_data.temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e056dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "calc_table = joined_data.select(sum(joined_data.inv_dist).alias(\"inv_dist_sum\"),\n",
    "                  sum(joined_data.temp_dist).alias('temp_dist_sum'))\n",
    "\n",
    "calc_table = calc_table.withColumn(\"temp_estimate\", calc_table.temp_dist_sum/calc_table.inv_dist_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413422d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+------------------+\n",
      "|       inv_dist_sum|    temp_dist_sum|     temp_estimate|\n",
      "+-------------------+-----------------+------------------+\n",
      "|0.22885390402133945|7.958959412036556|34.777468385658054|\n",
      "+-------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calc_table.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
