{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ec3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb40562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up the spark session and has magic code that makes things not break\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark = SparkSession.builder.appName('hw5').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259c44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "# Initiailizes the udf version of the haversine function\n",
    "haversineUDF = udf(lambda x, y: haversine(28.396837, -80.605659, x, y), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9538d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the stations from the csv\n",
    "stations = spark.read.csv('stations.csv', header=False, inferSchema=True).withColumnRenamed(\"_c0\", \"StationID\").withColumnRenamed(\"_c1\", \"WBANID\").withColumnRenamed(\"_c2\", \"GPSLatitude\").withColumnRenamed(\"_c3\", \"GPSLongitude\")\n",
    "\n",
    "# Creates a df with the distance from Cape Canaveral using the haversine function\n",
    "stations = stations.withColumn(\"distance\", haversineUDF(stations.GPSLatitude, stations.GPSLongitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67edc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.createOrReplaceTempView('stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab23db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+------------+---------+\n",
      "|StationID|WBANID|GPSLatitude|GPSLongitude| distance|\n",
      "+---------+------+-----------+------------+---------+\n",
      "|   722050| 12815|     28.434|     -81.325| 70.47143|\n",
      "|   722053| 12841|     28.545|     -81.333| 72.97912|\n",
      "|   747946| 12886|     28.617|     -80.683|25.620928|\n",
      "|   747950| 12867|     28.233|       -80.6|18.226263|\n",
      "|   722040| 12838|     28.101|     -80.644|33.109257|\n",
      "|   749047|  null|     28.283|     -81.416|    80.31|\n",
      "|   997806|  null|       28.4|     -80.533|7.1157584|\n",
      "|   720904|   299|     29.067|     -81.283| 99.57265|\n",
      "|   722056| 12834|     29.183|     -81.048| 97.46734|\n",
      "|   722051| 12841|     28.545|     -81.333| 72.97912|\n",
      "|   747945|  null|     28.617|       -80.7|  26.1591|\n",
      "|   998275|  null|     28.017|     -80.683|  42.9105|\n",
      "|   747870| 12834|     29.183|     -81.048| 97.46734|\n",
      "|   997354|  null|      28.42|      -80.58|3.5960674|\n",
      "|   995450|  null|     28.519|     -80.166| 45.07606|\n",
      "|   722046| 12898|     28.517|       -80.8|23.226759|\n",
      "|   747940| 12868|     28.483|     -80.567|10.299568|\n",
      "|   722058|  null|      29.07|      -80.92|80.883995|\n",
      "|   722361| 92808|     29.054|     -80.948| 80.33688|\n",
      "|   722011| 92813|      28.29|     -81.437| 82.22145|\n",
      "+---------+------+-----------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removes stations that have null coordinates or IDs, removes stations that are more than 100km away from Cape Canaveral\n",
    "# and drops duplicates\n",
    "stations = spark.sql(\"\"\"\n",
    "    SELECT * FROM STATIONS\n",
    "    WHERE GPSLatitude != 0 AND GPSLongitude != 0 AND distance <= 100 AND StationID IS NOT NULL\n",
    "\"\"\")\n",
    "stations = stations.dropDuplicates([\"StationID\"])\n",
    "stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6cc113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+---+----+\n",
      "|StationID|WBANID|Month|Day|temp|\n",
      "+---------+------+-----+---+----+\n",
      "|    10010|  null|    1|  1|17.2|\n",
      "|    10010|  null|    1|  2|12.1|\n",
      "|    10010|  null|    1|  3|10.4|\n",
      "|    10010|  null|    1|  4|17.4|\n",
      "|    10010|  null|    1|  5|26.5|\n",
      "|    10010|  null|    1|  6|30.1|\n",
      "|    10010|  null|    1|  7|29.7|\n",
      "|    10010|  null|    1|  8|29.6|\n",
      "|    10010|  null|    1|  9|29.6|\n",
      "|    10010|  null|    1| 10|33.0|\n",
      "|    10010|  null|    1| 11|32.5|\n",
      "|    10010|  null|    1| 12|27.4|\n",
      "|    10010|  null|    1| 13|22.2|\n",
      "|    10010|  null|    1| 14|11.3|\n",
      "|    10010|  null|    1| 15| 2.5|\n",
      "|    10010|  null|    1| 16| 3.0|\n",
      "|    10010|  null|    1| 17|13.4|\n",
      "|    10010|  null|    1| 18|29.8|\n",
      "|    10010|  null|    1| 19|27.5|\n",
      "|    10010|  null|    1| 20|25.2|\n",
      "+---------+------+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1986 = spark.read.csv('1986.csv', header=False, inferSchema=True).withColumnRenamed(\"_c0\", \"StationID\").withColumnRenamed(\"_c1\", \"WBANID\").withColumnRenamed(\"_c2\", \"Month\").withColumnRenamed(\"_c3\", \"Day\").withColumnRenamed(\"_c4\", \"temp\")\n",
    "data_1986.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ff4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1986.createOrReplaceTempView('_1986')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8fbb3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+---+----+\n",
      "|StationID|WBANID|Month|Day|temp|\n",
      "+---------+------+-----+---+----+\n",
      "|    10010|  null|    1|  1|17.2|\n",
      "|    10010|  null|    1|  2|12.1|\n",
      "|    10010|  null|    1|  3|10.4|\n",
      "|    10010|  null|    1|  4|17.4|\n",
      "|    10010|  null|    1|  5|26.5|\n",
      "|    10010|  null|    1|  6|30.1|\n",
      "|    10010|  null|    1|  7|29.7|\n",
      "|    10010|  null|    1|  8|29.6|\n",
      "|    10010|  null|    1|  9|29.6|\n",
      "|    10010|  null|    1| 10|33.0|\n",
      "|    10010|  null|    1| 11|32.5|\n",
      "|    10010|  null|    1| 12|27.4|\n",
      "|    10010|  null|    1| 13|22.2|\n",
      "|    10010|  null|    1| 14|11.3|\n",
      "|    10010|  null|    1| 15| 2.5|\n",
      "|    10010|  null|    1| 16| 3.0|\n",
      "|    10010|  null|    1| 17|13.4|\n",
      "|    10010|  null|    1| 18|29.8|\n",
      "|    10010|  null|    1| 19|27.5|\n",
      "|    10010|  null|    1| 20|25.2|\n",
      "+---------+------+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jan_1986 = spark.sql(\"\"\"\n",
    "    SELECT * FROM _1986\n",
    "    WHERE Month == 1\n",
    "\"\"\")\n",
    "jan_1986.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d8fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
